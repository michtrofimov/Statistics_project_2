---
title: "r_project_2"
author: "Michil Trofimov, Anastasia Gorabarenko, Polina Vaganova"
date: "2023-11-21"
output: html_document
---

# Libraries

```{r library, echo=TRUE, message=FALSE, warning=FALSE}
#if you don' have these new libraries, please uncomment (Shift+Ctrl+C) next 3 strings:
# install.packages("data.table")
# install.packages("dlookr")
# install.packages("flextable")
# install.packages("pROC")
library(data.table) # for faster data upload
library(pROC)
library(ggplot2)
library(dplyr)
library(car)
library(performance)
library(lme4)
library(corrplot)
library(dlookr) # for easier EDA
library(flextable) # for more beautiful tables
```

```{r main, include=FALSE}
main_dir <- dirname(rstudioapi::getSourceEditorContext()$path) 
setwd(main_dir)
```


# Dataset description

Данный датасет содержит информацию о полевых испытаниях ~300 сортов (id) сои в 2 локациях в течение 5 лет. Измерялись productivity (г/м^2), oil_content, protein_content (в процентах) и vegetation_period (время в днях от всхода до сбора). Кроме того известны форма листьев (leaf_shape), группа созревания (maturation_group, чем больше тем более позднеспелый), группа цветения (flowering_group), цвет опушения (pubescence_colour), цвет венчика (corolla_colour), страна происхождения (origin), полегание (lodging_type), тип роста (growth_type, индетерминантный - цветёт до сбора, детерминантный - цветёт один раз). 

2943 observations and 16 columns.

# Exploratory Data Analysis

## Analyse NA observations and outliers

```{r upload data, include=FALSE}
df = fread('soybean.csv', key='id', na.strings = c('',NA))
df = df[,-1]
```

```{r, echo=FALSE}
diagnose(df) %>% flextable()
```

We see that almost a half of observations are missing in numerical variables. Other than that, there are 108 missing observations in `origin` features.

First, we need to factorize id, leaf_shape, maturation_group, lodging_type, growth_type, flowering_group, pubescence_colour, corolla_colour, origin, site, year. 

Second, check missing observations in `origin`.

Third, remove missing observations in numerical features.

Fourth, analyse outliers.

1. Factorization 

```{r}
df = df %>%
  mutate(across(c('id', 'leaf_shape', 'maturation_group', 'lodging_type', 'growth_type', 'flowering_group', 'pubescence_colour', 'corolla_colour', 'origin', 'site', 'year'), as.factor))
```

2. NA in `origin`

Check missing observations in `origin`

```{r, echo=FALSE}
df[!complete.cases(df$origin)] %>% head %>% flextable() %>% autofit
```

We can notice that some observations have NA in `origin` column but not in numerical columns. It is pretty crucial, that is why we keep them. Thus, drop observations with NAs in both `origin` and numerical columns.

```{r, include=FALSE}
na_origin = df[!complete.cases(df$origin)]
na_origin_num_columns =  na_origin[!complete.cases(na_origin$productivity)]
v1_na_origin_num_columns = na_origin_num_columns$V1
df = df[!rownames(df) %in% v1_na_origin_num_columns]
```

3. NA in numerical columns

It would be more conservative to drop NA observations. We can impute them with mean of a breed (`id`), but some breeds have a lot of missing observations e.g. `id` = 1 has 7/9 NA in `productivity` column. And it is not crucial to fill NA, if a breed has very similar values in numerical column, for example: `id` = 8 has 6/8 NA in `productivity` but remaining two values are 51 and 53, mean = 52.

Due to that, let's drop missing values by `productivity` column.

```{r, include=FALSE}
df = df[complete.cases(df$productivity)]
```


4. Analysis of outliers

Check general descriptive statistics of numerical columns

```{r, echo=FALSE}
diagnose_numeric(df) %>% flextable()
```

Visualise numerical columns

```{r, echo=FALSE}
num_columns <- select(df, productivity, vegetation_period, protein_content, oil_content)
plot(num_columns)
```

How outliers affect our variables? 

```{r, echo=FALSE}
diagnose_outlier(df) %>%
  filter(outliers_cnt > 0) %>% flextable()
```

In all three variables outliers almost do not affect mean. These can be explained by the fact that outliers come from different breeds of plants, for example: extremely low producing breeds will generally give low `productivity` numbers.

## Feature analysis

```{r,echo=FALSE}
df %>% 
  correlate() %>% 
  plot()
```



```{r echo=FALSE}
diagnose_category(df) %>% flextable()
```

# Hypothesis checking

## First hypothesis

```{r Anova1_plot, plotfig.align="center", fig.height=5, fig.width=7, echo=FALSE}
ggplot(df, aes(x = flowering_group, y = productivity)) +
  geom_boxplot() +
  labs(title = "Boxplot of Productivity by Flowering Group",
       x = "Flowering Group",
       y = "Productivity")
ggplot(df, aes(x = flowering_group, y = productivity, fill = flowering_group)) +
  geom_violin() +
  labs(title = "Violin Plot of Productivity by Flowering Group",
       x = "Flowering Group",
       y = "Productivity") +
  theme_minimal()
```

The distributions do not appear to be normal. To confirm this, re-evaluate using the Shapiro test. If the 'is_normal' column contains 'Normal,' it implies that we cannot reject the hypothesis that the data follows a normal distribution

```{r number of observations in each group}
ggplot(df, aes(x = flowering_group)) +
  geom_bar()+
  labs(title = "Number of observations in flowering gorups",
       x = "Flowering Group",
       y = "Number of observations") +
  theme_classic()

```

We have uneven number of observations in flowering groups. It can reduce power of our ANOVA analysis, thus, it is better to stick to non-parametric Kruskal-Wallis test.

```{r shapiro_1}
shapiro_test_result <- df %>%
  group_by(flowering_group) %>%
  summarise(p_value = shapiro.test(productivity)$p.value)
shapiro_test_result$is_normal <- ifelse(shapiro_test_result$p_value < 0.05, "Not Normal", "Normal")

# Вывод результатов теста
print(shapiro_test_result)

```

The observations are independent and we mostly have normal distributions. Test for homogeneity of variances between groups will likely show that there is no homogeneity, which is expected, as we have several groups which do not follow normal distribution. It will not introduce a massive bias, but we should be careful when interpreting results.

In this case we can use ANOVA for this data. Let's do it!

```{r Anova, echo=FALSE, message=FALSE, warning=FALSE}
anova_result <- aov(productivity ~ flowering_group, data = df)
check_normality(anova_result)

check_homogeneity(anova_result)
print('Anova result')
summary(anova_result)
```

The ANOVA results show a significant difference among at least two of the groups in the variable 'productivity' based on the 'flowering_group' factor.
The p-value (Pr(>F)) is highly significant (less than 0.001), indicating that there are significant differences among the groups.

But residuals have not  normal distribution, in this case we will re-check our data with the Kruskal-Wallis test (It is a non-parametric alternative to one-way ANOVA that doesn't assume normality of the residuals)

```{r kruskal_test_1, echo=FALSE, message=FALSE, warning=FALSE}
kruskal_test_result <- kruskal.test(productivity ~ flowering_group, data = df)
print(kruskal_test_result)
```

The p-value is extremely low (p-value < 2.2e-16), providing strong evidence to reject the null hypothesis. There are significant differences in the medians of productivity among the different groups defined by the variable flowering_group.
 
**Post-Hoc Test:**

Since the Kruskal-Wallis test doesn't tell us which specific groups are different, check it via post hoc pairwise Wilcoxon test with bonferroni correction (because of multiple comparisons). Let's see on the plot with resulting p-values.

```{r posthoc1, echo=FALSE, message=FALSE, warning=FALSE}
posthoc1 <- pairwise.wilcox.test(df$productivity, df$flowering_group, p.adjust.method = "bonferroni")
pvals_posthoc1=posthoc1$p.value
corrplot(as.matrix(pvals_posthoc1), is.corr=F, method="number", col = COL2('RdBu'), type = 'lower',tl.srt = 60, tl.cex=1.5, number.cex = 1.2, cl.cex = 1.2, col.lim = c(0, 1))
```

From the output of the Wilcoxon rank sum test with Bonferron correction, it follows that there are statistically significant differences in performance (productivity) between the different flowering group variants. The p-values are below the 0.05 for most pairs of groups.

Specifically, p-values < 0.05 indicate that there is a statistically significant difference in performance between findings 2 and 1, 3 and 1, 3 and 1.5, 3.5 and 1, 3.5 and 1.5, 4 and 1, 4.5 and 1, 4.5 and 3, 5 and 2, 5 and 3-4.5. p-values=0 (extremely close to the 0) indicates very high significance values. 

Thus, it is possible to disable that the blooming_group group level affects productivity, and individual pairs of groups have characteristic significant differences.

## Second hypothesis

Let's visualize how changes oil content in different maturation groups (the more group number the more late-maturing). 

```{r Box_plot_test_2, echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE, plotfig.align="center"}
ggplot(df, aes(x = maturation_group, y = oil_content, fill = maturation_group)) +
  geom_boxplot(outlier.color = adjustcolor("black", alpha.f = 0)) +
  theme_classic() +
  labs(title = "Boxplot of Oil Content by Maturation Group",
       x = "Maturation Group",
       y = "Oil Content") +
  scale_fill_manual(values = c("#00BFFF", "#FFFF00",  "#00FF7F", "#F39C12", "#E74C3C"))

```

After looking at the graph, we propose our second hypothesis: Oil Content is different among Maturation Groups.  

Firstly need to see on distrubutions shapes to choose correct way for hypothesis testing.

```{r violin_plot_2, echo=FALSE, fig.height=5, fig.width=7, message=FALSE, warning=FALSE, plotfig.align="center"}
ggplot(df, aes(x = maturation_group, y = oil_content, fill = maturation_group)) +
  geom_violin() +
  labs(title = "Violin Plot of Productivity by Flowering Group",
       x = "Flowering Group",
       y = "Productivity") +
  theme_minimal() +
  labs(title = "Boxplot of Oil Content by Maturation Group",
       x = "Maturation Group",
       y = "Oil Content") +
  scale_fill_manual(values = c("#00BFFF", "#FFFF00",  "#00FF7F", "#F39C12", "#E74C3C"))

```

Again, seems the distributions do not appear to be normal. To confirm this we will use the Shapiro test. If the 'is_normal' column contains 'Normal,' it implies that we cannot reject the hypothesis that the data follows a normal distribution. 

```{r shapiro_2, echo=FALSE, message=FALSE, warning=FALSE}
shapiro_test_result <- df %>%
  group_by(maturation_group) %>%
  summarise(p_value = shapiro.test(oil_content)$p.value)
shapiro_test_result$is_normal <- ifelse(shapiro_test_result$p_value < 0.05, "Not Normal", "Normal")

# Вывод результатов теста
print(shapiro_test_result)

```

The observations are independent and mostly distributed not normal, thus, it is better to stick to non-parametric Kruskal-Wallis test. 
Let's see on the Kruskal-Wallis test results.

```{r kruskal_test_2, echo=FALSE, message=FALSE, warning=FALSE}
kruskal_test_result <- kruskal.test(oil_content ~ maturation_group, data = df)
print(kruskal_test_result)
```

The p-value is significantly low (p-value = 1.026e-15), providing strong evidence to reject the null hypothesis. There are significant differences in the means ranks of the Oil Content among the different Maturation Groups.

**Post-Hoc Test:**

Since the Kruskal-Wallis test doesn't tell us which specific groups are different, check it via post hoc pairwise Wilcoxon test with bonferroni correction (because of multiple comparisons). Let's see on the plot with resulting p-values.

```{r posthoc2, echo=FALSE, message=FALSE, warning=FALSE}
posthoc2 <- pairwise.wilcox.test(df$oil_content, df$maturation_group, p.adjust.method = "bonferroni")
pvals_posthoc2=posthoc2$p.value
corrplot(as.matrix(pvals_posthoc2),is.corr=F, method="number", col = COL2('RdBu'), type = 'lower',tl.srt = 60, tl.cex=1.5, number.cex = 1.5, cl.cex = 1.2, col.lim = c(0, 1))
```

From the output of the Wilcoxon rank sum test with Bonferron correction, it follows that there are statistically significant differences in Oil Content among some of Maturation Groups. 

Specifically, p-values < 0.05 indicate that there is a statistically significant difference in mean ranks of the Oil Content between maturation groups 1 and 2, 1 and 3, 1 and 4, 5 and 2, 5 and 3, 5 and 4. p-values=0 (extremely close to the 0) indicates very high significance values. 

Thus, it is possible to say that the Oil Maturation group level affects Oil Content, and individual pairs of groups have significant differences in Oil Content mean ranks.

# Linear models

## First model

```{r l1 , warning=FALSE}
lm_model_0 <- lm(productivity ~ protein_content, data = df)
lm_model_1 <- lm(productivity ~ oil_content, data = df)
lm_model <- lm(productivity ~ protein_content + oil_content, data = df)


aic_lm0 <- AIC(lm_model_0)
aic_lm1 <- AIC(lm_model_1)
aic_lm <- AIC(lm_model)

cat("AIC for Linear Model with only protein_content :", aic_lm0, "\n")
cat("AIC for Linear Model with only oil_content:", aic_lm1, "\n")
cat("AIC for Linear Mode with both factors:", aic_lm, "\n")
```

In this case, the model that includes both factors (protein_content and oil_content) has the smallest (though not by much) AIC value (13451.16), which means that it is preferable to models that include only one of these factors.
We add id as a dummy variable to account for repeated measures and compare with a mixed model that includes individual ids as random effects, accounting for the correlation between repeated measures for the same id.


```{r l2 , warning=FALSE}
lm_id_model <- lm(productivity ~ protein_content + oil_content + factor(id), data = df)
mixed_model <- lmer(productivity ~ protein_content + oil_content + (1 | id), data = df) 

aic_lm_id <- AIC(lm_id_model)
aic_mixed <- AIC(mixed_model)

cat("AIC for Linear Model with ID:", aic_lm_id, "\n")
cat("AIC for Mixed Model:", aic_mixed, "\n")

mixed_model <- lmer(productivity ~ protein_content + oil_content + (1 | id), data = df) 
summary(mixed_model)
```

Thus, the Linear Model with ID has the best AIC score and provides more information compared to the others.

Let's check with R step function for all this data:

```{r l3 , warning=FALSE, echo= FALSE}
full_model <- lm(productivity ~ protein_content + oil_content + factor(id), data = df)

step_model <- step(full_model, direction = "both", trace = 0)

summary(step_model)
```
Well, yes, the best call for such parameters was for Call:
lm(formula = productivity ~ protein_content + oil_content + factor(id), 
    data = df)

## Second model

We want to create a model to predict site of yielding depend on other data.

Fist, try to take all numerical data as model parameters:
```{r glm_site_1 , warning=FALSE, echo= FALSE}
df_without_na <- na.omit(df)
glm_site <- glm(site ~ productivity + vegetation_period + protein_content + oil_content, family = binomial, data = df_without_na)
summary(glm_site)
```

To estimate the classifier's ability of this model let's plot ROC curve (package pROC). 

It tells about model quality: correct model on its ROC curve always has a point close to (0, 1).
Another useful metric - AUC (Area under the ROC Curve). A properly working model has AUC close to 1.

```{r roc_curve_1 , warning=FALSE, echo= FALSE}
predicted_probs <- predict(glm_site, type = "response")
roc_curve <- roc(df_without_na$site, predicted_probs)
plot(roc_curve, col = "blue", main = "ROC Curve")
auc(roc_curve)
```
 
AUC = 0.8 means that model works pretty correct.

However, we are going to improve model predictive ability via adding new parameters: `growth_type`, `flowering_group`, 
`maturation_group`, `leaf_shape`, `corolla_colour`, `pubescence_colour`.

```{r glm_site_2 , warning=FALSE, echo= FALSE}
glm_site2 <- glm(site ~ productivity + vegetation_period + protein_content + oil_content + growth_type + flowering_group + maturation_group + leaf_shape + corolla_colour + pubescence_colour, family = binomial, data = df_without_na)
summary(glm_site2)
```

AIC of new model became smaller that means that new model now works better.

Check how it affect the ROC-curve.
```{r roc_curve_2 , warning=FALSE, echo= FALSE}
predicted_probs <- predict(glm_site2, type = "response")
roc_curve <- roc(df_without_na$site, predicted_probs)
plot(roc_curve, col = "blue", main = "ROC Curve")
auc(roc_curve)
```

AUC also become better. We can stop here because AUC=0.9015 means, 
that model correctly predict site of yielding in 90 % of cases and can be useful.


